import { generateWebLLMResponse, initWebLLM } from './webLlmService';

interface LLMSettings {
  apiKey: string;
  baseUrl: string;
  model: string;
  useWebLLM?: boolean;
  webLlmModel?: string;
}


const cleanLLMResponse = (text: string): string => {
  let cleaned = text.trim();

  // Remove common conversational prefixes
  const prefixes = [
    /^Here is the (transformed )?text:?\s*/i,
    /^Here is the (transformed )?script:?\s*/i,
    /^Transformed text:?\s*/i,
    /^Output:?\s*/i,
    /^Sure,? here is (the )?(transformed )?(text|script)( you requested)?:?\s*/i,
    /^Okay,? here is (the )?(transformed )?text:?\s*/i
  ];

  for (const prefix of prefixes) {
    cleaned = cleaned.replace(prefix, '');
  }

  // Remove markdown code blocks if present
  cleaned = cleaned.replace(/^```(markdown|text)?\n/, '').replace(/\n```$/, '');

  // Re-trim after prefix removal
  cleaned = cleaned.trim();

  // Remove wrapping quotes if they appear on both ends
  if ((cleaned.startsWith('"') && cleaned.endsWith('"')) || 
      (cleaned.startsWith("'") && cleaned.endsWith("'"))) {
      cleaned = cleaned.substring(1, cleaned.length - 1);
  }

  return cleaned.trim();
};

export const transformText = async (settings: LLMSettings, text: string): Promise<string> => {
  /* Shared System Prompt for both WebLLM and Remote API */
  const systemPrompt = `Transform the following slide text into a complete, natural-sounding script suitable for Text-to-Speech.
The original text is often fragmented (titles, bullets, metadata) and needs to be connected into coherent sentences.
Do not hallucinate new facts, but strictly "connect the dots" or "fill in the blanks" to make it flow well.

IMPORTANT TTS INSTRUCTIONS:
1. Expansion: Expand all technical abbreviations into their full spoken form to ensure correct pronunciation.
   - Example: "MiB/s" -> "mebibytes per second"
   - Example: "GB" -> "gigabytes"
   - Example: "vs." -> "versus"
   - Example: "etc." -> "et cetera"
2. Terminal Commands:
   - Do NOT read the leading '$' prompt symbol.
   - Break down complex commands into clear, spoken steps.
   - Spell out important symbols to ensure the listener knows exactly what to type.
   - Example: "$ git commit -m 'msg'" -> "First type git commit space dash m, then include your message in quotes."
   - Example: "$ npm install ." -> "Type npm install space period."
   - Example: "ls -la" -> "Type ls space dash l a."
3. Punctuation: Use proper punctuation to control pacing.
4. Clean Output: Return ONLY the raw string of the transformed text. 
   - Do NOT wrap the output in quotation marks. 
   - Do NOT include any prefixes like "Here is the transformed text:" or "Output:".
   - Do NOT use Markdown code blocks.

Example Input:
" How to Install Visual Studio Code on Windows A Complete Beginner's Guide Step-by-Step Instructions for First-Time Users  Windows 10/11  ~5 Minutes  Free & Open Source Download size: 85 MiB $ npm install ."

Example Output:
How to Install Visual Studio Code on Windows. This is a Complete Beginner's Guide including step-by-Step Instructions designed for First-Time Users. This guide is compatible with Windows 10 or Windows 11 operating systems. It will take around 5 minutes to complete. Visual Studio Code is free and open-source software, with a download size of approximately 85 mebibytes. To install dependencies, type npm install space period.`;

  const userPrompt = `Input Text:
"${text}"`;

  if (settings.useWebLLM) {
    if (!settings.webLlmModel) {
        throw new Error("WebLLM is enabled but no model is selected.");
    }
    try {
        // Ensure initialized. If not already loaded, this might take time.
        // We pass a simple console logger for this implicit init.
        await initWebLLM(settings.webLlmModel, (progress) => {
            console.log(`[WebLLM Auto-Init] ${progress.text}`);
        });

        const messages = [
            { role: "system" as const, content: systemPrompt },
            { role: "user" as const, content: userPrompt }
        ];

        const response = await generateWebLLMResponse(messages);
        return cleanLLMResponse(response);
    } catch (error) {
        console.error("WebLLM Error:", error);
        throw error;
    }
  }

  let endpoint = settings.baseUrl;
  // Ensure we hit the chat completions endpoint if not provided
  if (!endpoint.endsWith('/chat/completions')) {
     // Remove trailing slash if present
     endpoint = endpoint.replace(/\/+$/, '');
     endpoint = `${endpoint}/chat/completions`;
  }

  try {
    const response = await fetch(endpoint, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        'Authorization': `Bearer ${settings.apiKey}`,
      },
      body: JSON.stringify({
        model: settings.model,
        messages: [
            { role: "system", content: systemPrompt },
            { role: "user", content: userPrompt }
        ],
        temperature: 0.7
      }),
    });

    if (!response.ok) {
      const errorData = await response.json().catch(() => ({}));
      throw new Error(errorData.error?.message || `Failed to generate content: ${response.statusText}`);
    }

    const data = await response.json();
    const textContent = data.choices?.[0]?.message?.content || '';
    
    return cleanLLMResponse(textContent);
  } catch (error) {
    console.error('LLM API Error:', error);
    throw error;
  }
};
